{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3-4 Vladimir Saraikin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Method of Moments Estimator (MOM) for $ \\lambda $:\n",
    "   - $ \\bar{X} = \\frac{1}{n} \\sum_{k=1}^{n} X_k $\n",
    "   - First moment of Poisson: $ E[X] = \\lambda $\n",
    "   - Equating sample mean to theoretical mean: $ \\bar{X} = \\lambda $\n",
    "   - $ \\hat{\\lambda}_{MOM} = \\bar{X} $\n",
    "\n",
    "2) MLE $ \\lambda $:\n",
    "   - Likelihood function: $ L(\\lambda) = \\prod_{k=1}^{n} \\frac{e^{-\\lambda} \\lambda^{X_k}}{X_k!} $\n",
    "   - Log-likelihood function: $ \\ell(\\lambda) = \\sum_{k=1}^{n} (X_k \\log(\\lambda) - \\lambda - \\log(X_k!)) $\n",
    "   - Derivative of log-likelihood: $ \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\sum_{k=1}^{n} \\left(\\frac{X_k}{\\lambda} - 1\\right) $\n",
    "   - Setting derivative to zero: $ \\sum_{k=1}^{n} \\left(\\frac{X_k}{\\lambda} - 1\\right) = 0 $\n",
    "   - $ \\lambda $: $ \\hat{\\lambda}_{MLE} = \\bar{X} $\n",
    "\n",
    "3) Fisher information $ I(\\lambda) $:\n",
    "   - Second Derivative of Log-Likelihood: $ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} \\sum_{k=1}^{n} X_k $\n",
    "   - $ E\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] = -\\frac{n}{\\lambda} $\n",
    "   - $ I(\\lambda) = -E\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] = \\frac{n}{\\lambda} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Method of Moments Estimators (MOM) for $ a $ and $ b $:\n",
    "   - $ \\bar{X} = \\frac{1}{n} \\sum_{k=1}^{n} X_k $\n",
    "   - Theoretical mean of uniform: $ E[X] = \\frac{a + b}{2} $\n",
    "   - Theoretical variance of uniform: $ Var(X) = \\frac{(b - a)^2}{12} $\n",
    "   - $Var$: $ S^2 = \\frac{1}{n-1} \\sum_{k=1}^{n} (X_k - \\bar{X})^2 $\n",
    "   - Equating sample mean to theoretical Mean: $ \\bar{X} = \\frac{\\hat{a}_{MOM} + \\hat{b}_{MOM}}{2} $\n",
    "   - Equating sample variance to theoretical variance: $ S^2 = \\frac{(\\hat{b}_{MOM} - \\hat{a}_{MOM})^2}{12} $\n",
    "   - $ \\hat{a}_{MOM} $ and $ \\hat{b}_{MOM} $:\n",
    "     - $ \\hat{a}_{MOM} = \\bar{X} - \\sqrt{3S^2} $\n",
    "     - $ \\hat{b}_{MOM} = \\bar{X} + \\sqrt{3S^2} $\n",
    "\n",
    "2) MLE for $ a $ and $ b $:\n",
    "   - Likelihood function: $ L(a,b) = \\prod_{k=1}^{n} \\frac{1}{b-a} $ for $ a \\leq X_k \\leq b $\n",
    "   - Log-likelihood function: $ \\ell(a,b) = -n \\log(b-a) $\n",
    "   - Derivatives of log-likelihood:\n",
    "     - $ \\frac{\\partial \\ell(a,b)}{\\partial a} = \\frac{n}{b-a} $\n",
    "     - $ \\frac{\\partial \\ell(a,b)}{\\partial b} = -\\frac{n}{b-a} $\n",
    "\n",
    "     - $ \\hat{a}_{MLE} = \\min(X_1, X_2, \\ldots, X_n) $\n",
    "     - $ \\hat{b}_{MLE} = \\max(X_1, X_2, \\ldots, X_n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133.7742238943535,\n",
       " 12.14895266220986,\n",
       " (109.96271422654016, 157.58573356216684))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "mu = 5\n",
    "n = 100  # number of observations\n",
    "\n",
    "data = np.random.normal(mu, 1, n)\n",
    "\n",
    "# MLE for theta_hat\n",
    "theta_hat_mle = np.exp(data.mean())\n",
    "\n",
    "def delta_method(data: np.array, func: callable, func_prime: callable) -> float:\n",
    "    \"\"\" \n",
    "    applies the delta method to estimate the variance of a transformed variable.\n",
    "    \"\"\"\n",
    "    sample_var = np.var(data, ddof=1)\n",
    "    sample_size = len(data)\n",
    "    est = func(data.mean())\n",
    "    se_square = (func_prime(data.mean())**2) * (sample_var/sample_size)\n",
    "    return est, np.sqrt(se_square)\n",
    "\n",
    "# transformation function and its derivative for e^mean(data)\n",
    "transformation = lambda x: np.exp(x)\n",
    "transformation_prime = lambda x: np.exp(x)\n",
    "\n",
    "theta_hat, se_theta_hat = delta_method(data, transformation, transformation_prime)\n",
    "\n",
    "# 95% confidence interval for theta_hat using the normal distribution\n",
    "alpha = 0.05\n",
    "z_score = np.abs(stats.norm.ppf(alpha/2))\n",
    "ci_lower = theta_hat - z_score * se_theta_hat\n",
    "ci_upper = theta_hat + z_score * se_theta_hat\n",
    "\n",
    "# MLE, SE, and confidence interval\n",
    "theta_hat_mle, se_theta_hat, (ci_lower, ci_upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
