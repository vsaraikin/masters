{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1-2 Vladimir Saraikin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Bias of this estimator: $\\hat{\\theta}$ is defined as $Bias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$.\n",
    "   - $E[\\hat{\\theta}] = \\int_{0}^{\\theta} x \\cdot f_{\\hat{\\theta}}(x) \\, dx$\n",
    "     where $f_{\\hat{\\theta}}(x) = n x^{n-1} / \\theta^n$, the PDF of the maximum of $n$ uniform distributions.\n",
    "   - $E[\\hat{\\theta}] = \\int_{0}^{\\theta} x \\cdot \\frac{n x^{n-1}}{\\theta^n} \\, dx = \\frac{n}{\\theta^n} \\int_{0}^{\\theta} x^n \\, dx = \\frac{n}{\\theta^n} \\cdot \\frac{\\theta^{n+1}}{n+1} = \\frac{n \\theta}{n+1}$\n",
    "   - $Bias(\\hat{\\theta}) = \\frac{n \\theta}{n+1} - \\theta = \\theta \\left(\\frac{n}{n+1} - 1\\right) = -\\frac{\\theta}{n+1}$\n",
    "\n",
    "2) SE of the estimator:\n",
    "   - SE of an estimator is the STD of the estimator, $SE(\\hat{\\theta}) = \\sqrt{Var(\\hat{\\theta})}$.\n",
    "   - Variance of $\\hat{\\theta}$:\n",
    "     - $Var(\\hat{\\theta}) = E[\\hat{\\theta}^2] - (E[\\hat{\\theta}])^2$\n",
    "     - $E[\\hat{\\theta}^2]$:\n",
    "       $E[\\hat{\\theta}^2] = \\frac{n \\theta^2}{n+2}$\n",
    "     - $Var(\\hat{\\theta}) = \\frac{n \\theta^2}{n+2} - \\left(\\frac{n \\theta}{n+1}\\right)^2 = \\theta^2 \\left(\\frac{n}{n+2} - \\frac{n^2}{(n+1)^2}\\right)$\n",
    "   - SE: $SE(\\hat{\\theta}) = \\sqrt{Var(\\hat{\\theta})} = \\theta \\sqrt{\\frac{n+1}{n+2} - \\frac{n}{(n+1)^2}}$\n",
    "\n",
    "3) MSE of the estimator:\n",
    "   - $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n",
    "   - $MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + (Bias(\\hat{\\theta}))^2$\n",
    "   - $MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + \\left(-\\frac{\\theta}{n+1}\\right)^2 = \\theta^2 \\left(\\frac{n+1}{n+2} - \\frac{n}{(n+1)^2} + \\frac{1}{(n+1)^2}\\right)$\n",
    "\n",
    "4) Is this estimator consistent?\n",
    "   - An estimator is consistent if it converges in probability to the *true* parameter value as the sample size $n$ goes to infinity.\n",
    "   - $\\lim_{n \\to \\infty} Bias(\\hat{\\theta}) = 0 \\quad and \\quad \\lim_{n \\to \\infty} Var(\\hat{\\theta}) = 0$\n",
    "   - Thus, the estimator $\\hat{\\theta}$ is consistent.\n",
    "\n",
    "5) Is this estimator strongly consistent?\n",
    "   - An estimator is strongly consistent if it converges almost surely to the true parameter value as the sample size $n$ goes to infinity.\n",
    "     $\\lim_{n \\to \\infty} P(\\hat{\\theta} = \\theta) = 1$\n",
    "     The estimator $\\hat{\\theta}$ is strongly consistent because the probability that $\\hat{\\theta}$ equals $\\theta$ approaches 1 as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Bias of this estimator:\n",
    "   - Since $X_i \\sim U[0, \\theta]$, $E[X_i] = \\frac{\\theta}{2}$.\n",
    "   - $E[\\hat{\\theta}] = E\\left[2 \\cdot \\frac{1}{n} \\sum_{k=1}^{n} X_k \\right] = 2 E[X_i]$ because $X_i$ are identically distributed.\n",
    "   - $E[\\hat{\\theta}] = 2 \\cdot \\frac{\\theta}{2} = \\theta$.\n",
    "   - Therefore, $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta = \\theta - \\theta = 0$.\n",
    "\n",
    "2) SE of this estimator:\n",
    "   - Since the $X_i$ are i.i.d., $\\text{Var}(X_i) = \\frac{\\theta^2}{12}$.\n",
    "   - The $Var$ of the average of the $X_i$'s is $\\text{Var}\\left(\\frac{1}{n} \\sum_{k=1}^{n} X_k\\right) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\text{Var}(X_k) = \\frac{\\theta^2}{12n}$.\n",
    "   - $\\text{Var}(\\hat{\\theta}) = 4 \\cdot \\text{Var}\\left(\\frac{1}{n} \\sum_{k=1}^{n} X_k\\right) = \\frac{\\theta^2}{3n}$.\n",
    "   - $\\text{SE}(\\hat{\\theta}) = \\sqrt{\\text{Var}(\\hat{\\theta})} = \\sqrt{\\frac{\\theta^2}{3n}} = \\frac{\\theta}{\\sqrt{3n}}$.\n",
    "\n",
    "3) MSE of the estimator:\n",
    "   - $\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + (\\text{Bias}(\\hat{\\theta}))^2$.\n",
    "   - As $\\text{Bias}(\\hat{\\theta}) = 0$, $\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta})$.\n",
    "   - Therefore, $\\text{MSE}(\\hat{\\theta}) = \\frac{\\theta^2}{3n}$.\n",
    "\n",
    "4) Is this estimator consistent?\n",
    "   - An estimator is consistent if both $\\text{Bias}(\\hat{\\theta}) \\to 0$ and $\\text{Var}(\\hat{\\theta}) \\to 0$ as $n \\to \\infty$.\n",
    "   - Since $\\text{Bias}(\\hat{\\theta}) = 0$ and $\\text{Var}(\\hat{\\theta}) = \\frac{\\theta^2}{3n} \\to 0$ as $n \\to \\infty$, the estimator $\\hat{\\theta}$ is consistent.\n",
    "\n",
    "5) Is this estimator strongly consistent?\n",
    "   - For the estimator $\\hat{\\theta}$, $SLLN$ implies that $\\frac{1}{n} \\sum_{k=1}^{n} X_k \\to E[X_i]$ almost surely as $n \\to \\infty$.\n",
    "   - Since $E[X_i] = \\frac{\\theta}{2}$, $2 \\cdot \\frac{1}{n} \\sum_{k=1}^{n} X_k \\to \\theta$ almost surely as $n \\to \\infty$.\n",
    "   - Thus, the estimator $\\hat{\\theta}$ is strongly consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13581015157406193"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "n = 100  # observations\n",
    "m = 1000  # simulations\n",
    "confidence_level = 0.95\n",
    "\n",
    "# calculate epsilon for the DKW inequality\n",
    "epsilon = np.sqrt(np.log(2 / (1 - confidence_level)) / (2 * n))\n",
    "\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.967, 1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def ecdf(data):\n",
    "    \"\"\"\n",
    "    compute ECDF for a one-dimensional array of measurements\n",
    "    \"\"\"\n",
    "    # mumber of data points\n",
    "    n = len(data)\n",
    "    # x-data for the ECDF\n",
    "    x = np.sort(data)\n",
    "    # y-data for the ECDF\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return x, y\n",
    "\n",
    "contain_true_cdf = 0\n",
    "contain_ecdf = 0\n",
    "\n",
    "true_cdf = norm.cdf\n",
    "\n",
    "for _ in range(m):\n",
    "    samples = np.random.normal(0, 1, n)\n",
    "    x, y_ecdf = ecdf(samples)\n",
    "    lower_band = y_ecdf - epsilon\n",
    "    upper_band = y_ecdf + epsilon\n",
    "\n",
    "    # check if true CDF is within the bands\n",
    "    y_true_cdf = true_cdf(x)\n",
    "    if np.all((y_true_cdf >= lower_band) & (y_true_cdf <= upper_band)):\n",
    "        contain_true_cdf += 1\n",
    "\n",
    "    # check if ECDF is within its own band (this is guaranteed)\n",
    "    if np.all((y_ecdf >= lower_band) & (y_ecdf <= upper_band)):\n",
    "        contain_ecdf += 1\n",
    "\n",
    "(contain_true_cdf / m, contain_ecdf / m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
