{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 – Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Quick note:\n",
    "Comparasion and Python code is not completed, I know that is not exaxctly what was asked in the assignment. But it was just a lot in short time for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to U:\n",
      " [[  4.3180423  18.366312    9.453627 ]\n",
      " [  7.372599   -6.975644  -14.219867 ]\n",
      " [  8.444608   -9.701819  -12.073999 ]\n",
      " [  2.1707811  12.403317   13.254414 ]\n",
      " [  2.8677337 -16.268196  -22.240843 ]]\n",
      "Gradient with respect to V:\n",
      " [[ -0.8207928    0.17444533   3.725928     9.408623     5.452488  ]\n",
      " [-16.901579    18.467587   -17.322805   -21.162186   -14.764771  ]\n",
      " [ -4.9798985   12.033523   -15.780456   -22.070341   -21.243605  ]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "\n",
    "def J(U, V, Y, lambda_val):\n",
    "    return jnp.linalg.norm(jnp.dot(U, V) - Y, 'fro')**2 + lambda_val**2 * (jnp.linalg.norm(U, 'fro')**2 + jnp.linalg.norm(V, 'fro')**2)\n",
    "\n",
    "grad_J_U = grad(J, argnums=0)  # Gradient with respect to U\n",
    "grad_J_V = grad(J, argnums=1)  # Gradient with respect to V\n",
    "\n",
    "# Examples\n",
    "n, k = 5, 3\n",
    "lambda_val_example = 0.5\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "U_example = random.normal(key, (n, k))\n",
    "V_example = random.normal(key, (k, n))\n",
    "Y_example = random.normal(key, (n, n))\n",
    "\n",
    "grad_U_example = grad_J_U(U_example, V_example, Y_example, lambda_val_example)\n",
    "grad_V_example = grad_J_V(U_example, V_example, Y_example, lambda_val_example)\n",
    "\n",
    "print(\"Gradient with respect to U:\\n\", grad_U_example)\n",
    "print(\"Gradient with respect to V:\\n\", grad_V_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function\n",
    "$ J(U, V) = \\| UV - Y \\|_F^2 + \\frac{\\lambda}{2} (\\| U \\|_F^2 + \\| V \\|_F^2) $\n",
    "\n",
    "Gradient with respect to $ U $:\n",
    "\n",
    "1. Differentiate the first term $ \\| UV - Y \\|_F^2 $ with respect to $ U $:\n",
    "   $ \\frac{\\partial}{\\partial U} \\| UV - Y \\|_F^2 = \\frac{\\partial}{\\partial U} \\text{Tr}((UV - Y)^T(UV - Y)) $\n",
    "   Using the properties of trace and derivative, we get:\n",
    "   $ = 2(UV - Y)V^T $\n",
    "\n",
    "2. Differentiate the regularization term $ \\frac{\\lambda}{2} \\| U \\|_F^2 $ with respect to $ U $:\n",
    "   $ \\frac{\\partial}{\\partial U} \\frac{\\lambda}{2} \\| U \\|_F^2 = \\lambda U $\n",
    "\n",
    "Combining these, the gradient with respect to $ U $ is:\n",
    "$ \\nabla_U J(U, V) = 2(UV - Y)V^T + \\lambda U $\n",
    "\n",
    "Gradient with respect to $ V $:\n",
    "\n",
    "1. Differentiate the first term $ \\| UV - Y \\|_F^2 $ with respect to $ V $:\n",
    "   $ \\frac{\\partial}{\\partial V} \\| UV - Y \\|_F^2 = \\frac{\\partial}{\\partial V} \\text{Tr}((UV - Y)^T(UV - Y)) $\n",
    "   Similarly, we get:\n",
    "   $ = 2U^T(UV - Y) $\n",
    "\n",
    "2. Differentiate the regularization term $ \\frac{\\lambda}{2} \\| V \\|_F^2 $ with respect to $ V $:\n",
    "   $ \\frac{\\partial}{\\partial V} \\frac{\\lambda}{2} \\| V \\|_F^2 = \\lambda V $\n",
    "\n",
    "Combining these, the gradient with respect to $ V $ is:\n",
    "$ \\nabla_V J(U, V) = 2U^T(UV - Y) + \\lambda V $\n",
    "\n",
    "These gradients are used in optimization to find the values of $ U $ and $ V $ that minimize $ J(U, V) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function\n",
    "$ f(\\mathbf{w}) = \\sum_{i=1}^{m} \\log(1 + e^{y_i \\mathbf{w}^T \\mathbf{x}_i}) + \\frac{1}{2} \\| \\mathbf{w} \\|_2^2 $\n",
    "\n",
    "Gradient of $ f(\\mathbf{w}) $:\n",
    "\n",
    "The gradient is given by:\n",
    "$ \\nabla f(\\mathbf{w}) = \\sum_{i=1}^{m} \\frac{y_i \\mathbf{x}_i e^{y_i \\mathbf{w}^T \\mathbf{x}_i}}{1 + e^{y_i \\mathbf{w}^T \\mathbf{x}_i}} + \\mathbf{w} $\n",
    "\n",
    "Hessian of $ f(\\mathbf{w}) $:\n",
    "\n",
    "The Hessian is given by:\n",
    "$ H(f(\\mathbf{w})) = \\sum_{i=1}^{m} \\frac{y_i^2 \\mathbf{x}_i \\mathbf{x}_i^T e^{y_i \\mathbf{w}^T \\mathbf{x}_i}}{(1 + e^{y_i \\mathbf{w}^T \\mathbf{x}_i})^2} + I $\n",
    "where $ I $ is the identity matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient:\n",
      " [ 0.10853742 -1.8221643  -0.919782    1.1513234   0.26755393]\n",
      "Hessian:\n",
      " [[ 1.8347886   0.16822092 -0.48792958 -0.17042848  0.05137864]\n",
      " [ 0.16822092  1.2593102  -0.00693707  0.0586279   0.03631498]\n",
      " [-0.48792958 -0.00693707  1.4965051   0.25343034 -0.08942212]\n",
      " [-0.17042848  0.0586279   0.25343034  1.3799262  -0.00960475]\n",
      " [ 0.05137864  0.03631499 -0.08942211 -0.00960474  1.3466067 ]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, hessian, random\n",
    "\n",
    "def f(w, X, y):\n",
    "    # X is a matrix of shape (m, n) where each row is x_i\n",
    "    # y is a vector of length m\n",
    "    log_terms = jnp.log1p(jnp.exp(jnp.dot(X, w) * y))\n",
    "    regularization = 0.5 * jnp.sum(w**2)\n",
    "    return jnp.sum(log_terms) + regularization\n",
    "\n",
    "grad_f = grad(f, argnums=0)\n",
    "\n",
    "hessian_f = hessian(f, argnums=0)\n",
    "\n",
    "m, n = 10, 5  # Example dimensions\n",
    "key = random.PRNGKey(0) \n",
    "w_example = random.normal(key, (n,)) \n",
    "X_example = random.normal(key, (m, n)) \n",
    "y_example = random.normal(key, (m,))\n",
    "\n",
    "grad_f_example = grad_f(w_example, X_example, y_example)\n",
    "hessian_f_example = hessian_f(w_example, X_example, y_example)\n",
    "\n",
    "print(\"Gradient:\\n\", grad_f_example)\n",
    "print(\"Hessian:\\n\", hessian_f_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function\n",
    "$ g(\\mathbf{x}) = f(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) $\n",
    "\n",
    "where $ \\nabla f(\\mathbf{y}) $ denotes the gradient of $ f $ at any point $ \\mathbf{y} $, the gradient of $ g $ with respect to $ \\mathbf{x} $ is given by:\n",
    "\n",
    "$ \\nabla g(\\mathbf{x}) = \\mathbf{A}^T \\nabla f(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function\n",
    "$ g(\\mathbf{x}) = f(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) $\n",
    "\n",
    "where $ \\nabla f(\\mathbf{y}) $ denotes the gradient of $ f $ at any point $ \\mathbf{y} $, the gradient of $ g $ with respect to $ \\mathbf{x} $ is given by:\n",
    "\n",
    "$ \\nabla g(\\mathbf{x}) = \\mathbf{A}^T \\nabla f(\\mathbf{A}\\mathbf{x} + \\mathbf{b}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Gradient of $ f(X) = \\sum_{i=1}^{n} \\lambda_i(X) $:\n",
    "\n",
    "Since the sum of the eigenvalues $ \\lambda_i(X) $ of a matrix $ X $ is equal to its trace:\n",
    "$ f(X) = \\text{Tr}(X) $\n",
    "\n",
    "The gradient of the trace of a matrix with respect to the matrix is the identity matrix:\n",
    "$ \\nabla_X f(X) = I $\n",
    "where $ I $ is the identity matrix of the same size as $ X $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Gradient of $ f(X) = \\prod_{i=1}^{n} \\lambda_i(X) $:\n",
    "\n",
    "The product of the eigenvalues $ \\lambda_i(X) $ of a matrix $ X $ is equal to its determinant:\n",
    "$ f(X) = \\det(X) $\n",
    "\n",
    "For a matrix $ X $ with distinct eigenvalues, the gradient of the determinant can be expressed in terms of the adjugate of $ X $:\n",
    "$ \\nabla_X f(X) = \\text{adj}(X)^T $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "def softmax(w):\n",
    "    w_max = jnp.max(w)\n",
    "    w_stable = w - w_max\n",
    "    e_w = jnp.exp(w_stable)\n",
    "    return e_w / jnp.sum(e_w)\n",
    "\n",
    "def f(w):\n",
    "    return softmax(w)\n",
    "\n",
    "jacobi_matrix = vmap(grad(f), in_axes=(0,))\n",
    "\n",
    "w_example = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "jacobi_matrix_example = jacobi_matrix(w_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.0000001e+00, -4.8470916e-09],\n",
       "       [-4.8470916e-09,  1.0000001e+00]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_eigenvalues(X):\n",
    "    eig_vals, _ = jnp.linalg.eigh(X)\n",
    "    return jnp.sum(eig_vals)\n",
    "\n",
    "grad_sum_eigenvalues = grad(sum_of_eigenvalues)\n",
    "\n",
    "X_example = jnp.array([[1.0, 2.0], [2.0, 3.0]])\n",
    "\n",
    "grad_sum_eigenvalues_example = grad_sum_eigenvalues(X_example)\n",
    "grad_sum_eigenvalues_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 3., -2.],\n",
       "       [-2.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def product_of_eigenvalues(X):\n",
    "    eig_vals, _ = jnp.linalg.eigh(X)\n",
    "    return jnp.prod(eig_vals)\n",
    "\n",
    "grad_product_eigenvalues = grad(product_of_eigenvalues)\n",
    "\n",
    "grad_product_eigenvalues_example = grad_product_eigenvalues(X_example)\n",
    "grad_product_eigenvalues_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
